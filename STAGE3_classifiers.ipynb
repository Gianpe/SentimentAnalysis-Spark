{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import isnan, when, count, col, split, regexp_replace, lower\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import Word2Vec,StringIndexer,CountVectorizer,HashingTF, IDF\n",
    "from pyspark.ml.classification import NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import subprocess\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[*]\").appName(\"ddam-8_STEP-3_w2v_classifiers\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(df, colname):\n",
    "    return df.withColumn(colname, split(lower(col(colname)), \" \")).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(df, filename):\n",
    "    filepath = f\"hdfs://kddrtserver12.isti.cnr.it:9000/user/hpsa06/{filename}\"\n",
    "    subprocess.call([\"hadoop\", \"fs\", \"-rm\", \"-r\", filepath])\n",
    "    df.write.csv(filepath, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pred):\n",
    "    evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "\n",
    "    predictionAndTarget = pred.select(\"label\", \"prediction\")\n",
    "    acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "    f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "    weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "    weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "    print('\\taccuracy:', acc)\n",
    "    print('\\tf1-score',f1)\n",
    "    print('\\tweightedPrecision',weightedPrecision)\n",
    "    print('\\tweightedRecall',weightedRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_df(df):\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "    scalerModel = scaler.fit(df)\n",
    "    return scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation algs. definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary feature length for all vectorizers - equal chances B-)\n",
    "f_len = 400\n",
    "\n",
    "def word2vec(df):\n",
    "    word2Vec = Word2Vec(vectorSize=f_len, minCount=2, inputCol=\"text\", outputCol=\"features\")\n",
    "    model = word2Vec.fit(df)\n",
    "    return model.transform(df)\n",
    "\n",
    "def bow(df):\n",
    "    bow = CountVectorizer(inputCol=\"text\", outputCol=\"features\", vocabSize=f_len, minDF=5).fit(df)\n",
    "    return bow.transform(df)\n",
    "    \n",
    "def tfidf(df):\n",
    "    hashingTF = HashingTF(inputCol=\"text\", outputCol=\"rawFeatures\", numFeatures=f_len)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    featurizedData = hashingTF.transform(df)\n",
    "    return idf.fit(featurizedData).transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "def nb(train, test):\n",
    "    DF_train = scaler_df(train)\n",
    "    DF_test = scaler_df(test)\n",
    "    model_nb = NaiveBayes(featuresCol='scaledfeatures', labelCol='label')\n",
    "    return model_nb.fit(DF_train).transform(DF_test)\n",
    "\n",
    "# Multilayer perceptron\n",
    "def mlp(train, test):\n",
    "    model_mlp = MultilayerPerceptronClassifier(layers=[f_len, int((f_len+3)/2), 3], solver='l-bfgs',\n",
    "                                               featuresCol='features', labelCol='label')\n",
    "    return model_mlp.fit(train).transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = ss.read.csv(\"hdfs://kddrtserver12.isti.cnr.it:9000/user/hpsa06/df_Model.csv\", sep=\",\", quote='\"', header=True, escape='\"', multiLine=True)\n",
    "trump_df = ss.read.csv(\"hdfs://kddrtserver12.isti.cnr.it:9000/user/hpsa06/df_Trump.csv\", sep=\",\", quote='\"', header=True, escape='\"', multiLine=True).drop('lat','long', 'city')\n",
    "biden_df = ss.read.csv(\"hdfs://kddrtserver12.isti.cnr.it:9000/user/hpsa06/df_Model.csv\", sep=\",\", quote='\"', header=True, escape='\"', multiLine=True).drop('lat','long', 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = split_text(twitter_df,'text')\n",
    "twitter_df = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\").fit(twitter_df).transform(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec :  nb\n",
      "\taccuracy: 0.47127909459748024\n",
      "\tf1-score 0.4040379308471977\n",
      "\tweightedPrecision 0.5712509642738162\n",
      "\tweightedRecall 0.4712790945974803\n",
      "word2vec :  mlp\n",
      "\taccuracy: 0.5558402733290626\n",
      "\tf1-score 0.5501303573469899\n",
      "\tweightedPrecision 0.5645962063482722\n",
      "\tweightedRecall 0.5558402733290626\n",
      "bow :  nb\n",
      "\taccuracy: 0.6234594135146622\n",
      "\tf1-score 0.6169300153222861\n",
      "\tweightedPrecision 0.6561517606665785\n",
      "\tweightedRecall 0.6234594135146622\n",
      "bow :  mlp\n",
      "\taccuracy: 0.6419464513387165\n",
      "\tf1-score 0.6412290389038638\n",
      "\tweightedPrecision 0.6576384962344565\n",
      "\tweightedRecall 0.6419464513387165\n",
      "tfidf :  nb\n",
      "\taccuracy: 0.5329849771391247\n",
      "\tf1-score 0.5175768915558268\n",
      "\tweightedPrecision 0.5701901349730272\n",
      "\tweightedRecall 0.5329849771391247\n",
      "tfidf :  mlp\n",
      "\taccuracy: 0.5536686261702591\n",
      "\tf1-score 0.5532564477835126\n",
      "\tweightedPrecision 0.5530715881427554\n",
      "\tweightedRecall 0.553668626170259\n"
     ]
    }
   ],
   "source": [
    "splits = [0.85, 0.15]\n",
    "\n",
    "for vec_alg in [word2vec, bow, tfidf]:\n",
    "    datasets = vec_alg(twitter_df).randomSplit(splits)\n",
    "    for model in [nb,mlp]:\n",
    "        print(vec_alg.__name__, ': ', model.__name__)\n",
    "        evaluate_model(model(*datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
